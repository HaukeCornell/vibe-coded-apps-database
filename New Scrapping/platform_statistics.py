# -*- coding: utf-8 -*-
"""platform_statistics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11B8U1w06lDkzv9Jj20oSfIrbv5S1Sh5D
"""

#!/usr/bin/env python3
"""
update_dashboard_from_new_scrapping.py

- Reads all CSV files in "New Scrapping/".
- Inserts new unique (platform, website_url) rows into vibe_coded_apps.db.
- Creates/updates the SQL view `platform_statistics`.
- Writes export/platform_statistics.csv and updates top-level README.md summary.
"""

import os
import sqlite3
import pandas as pd
from datetime import datetime

# CONFIG â€” adjust if your repo uses different names
NEW_SCRAPING_DIR = "New Scrapping"         # folder in repo with incoming CSVs
DB_PATH = "vibe_coded_apps.db"             # DB at repo root
EXPORT_DIR = "export"                      # where CSV exports go
README_PATH = "README.md"                  # top-level readme to update/write
EXPECTED_COLS = {"platform", "website_url"}  # required columns in incoming CSVs

os.makedirs(EXPORT_DIR, exist_ok=True)

def find_csv_files(folder):
    files = []
    if not os.path.exists(folder):
        return files
    for fname in sorted(os.listdir(folder)):
        if fname.lower().endswith(".csv"):
            files.append(os.path.join(folder, fname))
    return files

def read_and_normalize_csv(path):
    df = pd.read_csv(path, dtype=str).fillna("")
    # make columns lower-case for safety
    df.columns = [c.strip() for c in df.columns]
    return df

def ensure_scrapes_table(conn):
    conn.execute("""
    CREATE TABLE IF NOT EXISTS scrapes (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        platform TEXT NOT NULL,
        website_url TEXT NOT NULL,
        scraped_at TEXT,
        source_file TEXT,
        UNIQUE(platform, website_url) ON CONFLICT IGNORE
    );
    """)
    conn.commit()

def insert_new_rows(conn, df, source_file):
    now = datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S")
    inserted = 0
    cur = conn.cursor()
    for _, row in df.iterrows():
        platform = row.get("platform") or row.get("Platform") or ""
        website = row.get("website_url") or row.get("website") or row.get("url") or ""
        platform = str(platform).strip()
        website = str(website).strip()
        if not platform or not website:
            continue
        try:
            cur.execute(
                "INSERT OR IGNORE INTO scrapes (platform, website_url, scraped_at, source_file) VALUES (?, ?, ?, ?)",
                (platform, website, now, source_file)
            )
            if cur.rowcount == 1:
                inserted += 1
        except Exception as e:
            print(f"Warning: failed to insert row ({platform}, {website}): {e}")
    conn.commit()
    return inserted

def recreate_platform_statistics_view(conn):
    conn.execute("DROP VIEW IF EXISTS platform_statistics;")
    conn.execute("""
    CREATE VIEW platform_statistics AS
    SELECT
        platform,
        COUNT(DISTINCT website_url) AS unique_websites,
        MIN(scraped_at) AS first_scraped,
        MAX(scraped_at) AS last_scraped
    FROM scrapes
    GROUP BY platform
    ORDER BY unique_websites DESC;
    """)
    conn.commit()

def export_stats_to_csv(conn, out_csv):
    df = pd.read_sql("SELECT * FROM platform_statistics;", conn)
    df.to_csv(out_csv, index=False)
    return df

def update_readme_with_stats(df, readme_path):
    header = "# ðŸ§© Vibe Coded Apps Database â€” Platform Statistics\n\n"
    header += f"**Auto-generated:** {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S (UTC)')}\n\n"
    header += "## Platform Statistics (unique websites per platform)\n\n"
    if df.empty:
        header += "_No data yet._\n"
    else:
        header += df.to_markdown(index=False)
        header += "\n\n"
    header += "---\n*This section is auto-generated by `tools/update_dashboard_from_new_scrapping.py`.*\n\n"

    # Read existing README if we want to preserve other parts. We'll replace any previous auto-generated block.
    if os.path.exists(readme_path):
        with open(readme_path, "r", encoding="utf-8") as f:
            original = f.read()
    else:
        original = ""

    # We will place the auto-generated block between markers so repeated runs replace only that block
    START_MARKER = "<!-- PLATFORM_STATS_START -->"
    END_MARKER = "<!-- PLATFORM_STATS_END -->"

    new_block = f"{START_MARKER}\n{header}\n{END_MARKER}\n"

    if START_MARKER in original and END_MARKER in original:
        # replace the block
        before = original.split(START_MARKER)[0]
        after = original.split(END_MARKER)[1]
        new_readme = before + new_block + after
    else:
        # prepend the generated block at the top
        new_readme = new_block + "\n" + original

    with open(readme_path, "w", encoding="utf-8") as f:
        f.write(new_readme)

def main():
    csv_files = find_csv_files(NEW_SCRAPING_DIR)
    print(f"Found {len(csv_files)} CSV file(s) in '{NEW_SCRAPING_DIR}'")

    if not csv_files:
        print("No new files to process. Exiting.")
        return

    conn = sqlite3.connect(DB_PATH)
    ensure_scrapes_table(conn)

    total_inserted = 0
    for csv_path in csv_files:
        print(f"Processing {csv_path} ...")
        try:
            df = read_and_normalize_csv(csv_path)
        except Exception as e:
            print(f"Error reading {csv_path}: {e}; skipping.")
            continue

        cols = set(df.columns)
        # try to map columns if names are slightly different
        if not (EXPECTED_COLS & cols):
            # look for common alternatives
            if ("url" in cols and "platform" in cols) or ("website" in cols and "platform" in cols):
                pass
            else:
                print(f"Warning: expected columns {EXPECTED_COLS} not found in {csv_path}. Found columns: {cols}. Attempting to continue.")
        inserted = insert_new_rows(conn, df, os.path.basename(csv_path))
        print(f"Inserted {inserted} new row(s) from {os.path.basename(csv_path)}")
        total_inserted += inserted

    print(f"Total new rows inserted: {total_inserted}")

    # create/update view
    recreate_platform_statistics_view(conn)
    print("platform_statistics view recreated.")

    # export CSV
    out_csv = os.path.join(EXPORT_DIR, "platform_statistics.csv")
    df_stats = export_stats_to_csv(conn, out_csv)
    print(f"Exported platform_statistics to {out_csv}")

    # update README.md (top-level)
    update_readme_with_stats(df_stats, README_PATH)
    print(f"Updated {README_PATH} with platform statistics section.")

    conn.close()
    print("Done.")

if __name__ == "__main__":
    main()